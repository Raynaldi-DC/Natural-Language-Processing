{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9z82JFF_g7c",
        "outputId": "6fc9a8a0-9ad3-43bc-c109-022168a669d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install library yang diperlukan\n",
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "!pip install datasets\n",
        "\n",
        "\n",
        "# Import library\n",
        "import os\n",
        "os.environ['WANDB_DISABLED'] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "\n",
        "# Kalau Pakai Colab :\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/TRAM/Data/single_label.json'  # Sesuaikan path Google Drive\n",
        "\n",
        "# # Kalau Pakai laptop :\n",
        "# file_path = '/TRAM/Data/single_label.json'  # Sesuaikan path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "df = pd.read_json(file_path)\n",
        "print(len(df))\n",
        "# df = df.head(500) # <<< Jumlah baris ============================================================\n",
        "\n",
        "# 2. Pembersihan Data (basic)\n",
        "df = df[['text', 'label']]  # Pastikan hanya kolom 'text' dan 'label'\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 3. Encoding Label\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# ========= Buang Label Terlalu Sedikit =========\n",
        "# Hitung jumlah data per label\n",
        "label_counts = df['label_encoded'].value_counts()\n",
        "# Ambil hanya label yang jumlahnya >= 2\n",
        "valid_labels = label_counts[label_counts >= 2].index\n",
        "# Filter dataframe\n",
        "df = df[df['label_encoded'].isin(valid_labels)]\n",
        "\n",
        "\n",
        "# 4. Membuat Train-Test Split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'].tolist(),\n",
        "    df['label_encoded'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label_encoded']\n",
        ")\n",
        "\n",
        "# 5. Load Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 6. Tokenisasi\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# 7. Dataset Custom\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "\n",
        "# 8. Load Model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(label_encoder.classes_)\n",
        ")\n",
        "\n",
        "# 9. Buat Folder Output di Google Drive\n",
        "output_dir = '/content/drive/MyDrive/TRAM/Output_BERT'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 10. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\", # Kalau versi baru ganti jadi evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f'{output_dir}/logs',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# 11. Metric (Optional Evaluation Metric untuk Trainer)\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    return {\n",
        "        'accuracy': (preds == labels).mean()\n",
        "    }\n",
        "\n",
        "# 12. Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 13. Train\n",
        "trainer.train()\n",
        "\n",
        "# # 14. Evaluasi\n",
        "# preds_output = trainer.predict(val_dataset)\n",
        "# predictions = np.argmax(preds_output.predictions, axis=1)\n",
        "# print(classification_report(val_labels, predictions, target_names=label_encoder.classes_))\n",
        "\n",
        "# 14. Evaluasi\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "preds_output = trainer.predict(val_dataset)\n",
        "predictions = np.argmax(preds_output.predictions, axis=1)\n",
        "\n",
        "# Temukan label yang ada di data validasi\n",
        "val_label_ids = unique_labels(val_labels, predictions)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(\n",
        "    val_labels,\n",
        "    predictions,\n",
        "    labels=val_label_ids,\n",
        "    target_names=label_encoder.inverse_transform(val_label_ids)\n",
        "))\n",
        "\n",
        "\n",
        "# 15. Save Model dan Label Encoder\n",
        "model.save_pretrained(f'{output_dir}/bert_model')\n",
        "tokenizer.save_pretrained(f'{output_dir}/bert_model')\n",
        "\n",
        "import pickle\n",
        "with open(f'{output_dir}/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"Model dan tokenizer berhasil disimpan di Google Drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eAkzY6J-SvH-",
        "outputId": "c92f245c-5555-4248-a194-b0e1f7c98443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2036' max='2036' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2036/2036 6:23:05, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.928300</td>\n",
              "      <td>1.656733</td>\n",
              "      <td>0.641454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.045400</td>\n",
              "      <td>0.743562</td>\n",
              "      <td>0.834971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.446900</td>\n",
              "      <td>0.598152</td>\n",
              "      <td>0.859528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.578810</td>\n",
              "      <td>0.864440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   T1003.001       1.00      1.00      1.00        22\n",
            "       T1005       0.83      0.77      0.80        13\n",
            "       T1012       1.00      0.20      0.33         5\n",
            "       T1016       0.89      0.73      0.80        11\n",
            "   T1021.001       0.85      0.89      0.87        19\n",
            "       T1027       0.86      0.91      0.88       137\n",
            "       T1033       0.80      0.80      0.80        10\n",
            "   T1036.005       0.77      0.71      0.74        14\n",
            "       T1041       0.65      0.69      0.67        16\n",
            "       T1047       1.00      1.00      1.00        15\n",
            "   T1053.005       1.00      1.00      1.00        21\n",
            "       T1055       0.95      0.95      0.95        57\n",
            "   T1056.001       1.00      0.92      0.96        13\n",
            "       T1057       0.86      0.75      0.80        16\n",
            "   T1059.003       0.93      0.94      0.94        69\n",
            "       T1068       0.00      0.00      0.00         2\n",
            "   T1070.004       0.65      0.88      0.75        17\n",
            "   T1071.001       0.82      0.79      0.81        29\n",
            "       T1072       0.00      0.00      0.00         2\n",
            "   T1074.001       0.00      0.00      0.00         5\n",
            "       T1078       0.86      0.94      0.90        34\n",
            "       T1082       0.86      0.93      0.89        27\n",
            "       T1083       0.81      0.85      0.83        20\n",
            "       T1090       0.93      0.93      0.93        28\n",
            "       T1095       0.59      1.00      0.74        10\n",
            "       T1105       0.84      0.79      0.81        52\n",
            "       T1106       0.91      0.75      0.82        40\n",
            "       T1110       0.81      1.00      0.90        13\n",
            "       T1112       0.75      0.86      0.80        21\n",
            "       T1113       0.75      0.90      0.82        10\n",
            "       T1140       0.99      0.95      0.97        91\n",
            "       T1190       0.67      1.00      0.80        10\n",
            "   T1204.002       0.94      0.88      0.91        17\n",
            "       T1210       0.00      0.00      0.00         3\n",
            "   T1218.011       1.00      1.00      1.00        11\n",
            "       T1219       0.67      0.80      0.73        10\n",
            "   T1484.001       1.00      0.80      0.89         5\n",
            "   T1518.001       0.62      0.71      0.67         7\n",
            "   T1543.003       0.60      0.90      0.72        10\n",
            "   T1547.001       0.92      0.92      0.92        13\n",
            "   T1548.002       1.00      1.00      1.00         5\n",
            "   T1552.001       0.00      0.00      0.00         4\n",
            "   T1557.001       0.00      0.00      0.00         1\n",
            "   T1562.001       0.83      0.79      0.81        19\n",
            "   T1564.001       1.00      1.00      1.00         3\n",
            "   T1566.001       0.89      1.00      0.94        17\n",
            "   T1569.002       0.00      0.00      0.00         5\n",
            "       T1570       0.71      0.83      0.77        12\n",
            "   T1573.001       0.75      0.55      0.63        11\n",
            "   T1574.002       1.00      0.88      0.93        16\n",
            "\n",
            "    accuracy                           0.86      1018\n",
            "   macro avg       0.73      0.74      0.72      1018\n",
            "weighted avg       0.85      0.86      0.86      1018\n",
            "\n",
            "Model dan tokenizer berhasil disimpan di Google Drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}